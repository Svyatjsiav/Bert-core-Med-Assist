import torch
import os
import json
import glob
import re
import numpy as np
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModel, pipeline
from sentence_transformers import SentenceTransformer
import tkinter as tk
from tkinter import filedialog
import shutil
from typing import List, Dict, Any, Tuple
import warnings
warnings.filterwarnings('ignore')


class MedicalAssistant:
    def __init__(self, model_name="DeepPavlov/rubert-base-cased", device="cpu"):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –Ω–∞ BERT
        
        Args:
            model_name (str): –ù–∞–∑–≤–∞–Ω–∏–µ BERT –º–æ–¥–µ–ª–∏ –∏–ª–∏ –ø—É—Ç—å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏
            device (str): –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π ('cpu' –∏–ª–∏ 'cuda')
        """
        self.device = torch.device(device if torch.cuda.is_available() and device == "cuda" else "cpu")
        self.model_name = model_name
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π
        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ BERT –º–æ–¥–µ–ª–∏: {model_name} –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ {self.device}")
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ (Sentence Transformers)
        try:
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-mpnet-base-v2')
            self.embedding_model.to(self.device)
        except:
            # Fallback –Ω–∞ –º–µ–Ω—å—à—É—é –º–æ–¥–µ–ª—å
            self.embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            self.embedding_model.to(self.device)
        
        # –ú–æ–¥–µ–ª—å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Ç–µ–∫—Å—Ç–∞
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(model_name)
            self.generation_model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
            self.generation_model.to(self.device)
        except:
            # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å –Ω–µ Seq2Seq, –∏—Å–ø–æ–ª—å–∑—É–µ–º –¥—Ä—É–≥—É—é
            print(f"‚ö†Ô∏è  –ú–æ–¥–µ–ª—å {model_name} –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É...")
            self.tokenizer = AutoTokenizer.from_pretrained("IlyaGusev/fred_t5_ru_turbo_alpaca")
            self.generation_model = AutoModelForSeq2SeqLM.from_pretrained("IlyaGusev/fred_t5_ru_turbo_alpaca")
            self.generation_model.to(self.device)
        
        # –ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –≤–æ–ø—Ä–æ—Å-–æ—Ç–≤–µ—Ç (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        try:
            self.qa_pipeline = pipeline(
                "question-answering",
                model="DeepPavlov/rubert-base-cased-squad",
                tokenizer="DeepPavlov/rubert-base-cased-squad",
                device=0 if str(self.device) == "cuda" else -1
            )
        except:
            self.qa_pipeline = None
        
        self.conversation_history = []
        self.patient_data = {}
        self.vault_content = []
        self.vault_embeddings = None
        
        # –¶–≤–µ—Ç–∞ –¥–ª—è –≤—ã–≤–æ–¥–∞ –≤ –∫–æ–Ω—Å–æ–ª—å
        self.PINK = '\033[95m'
        self.CYAN = '\033[96m'
        self.YELLOW = '\033[93m'
        self.NEON_GREEN = '\033[92m'
        self.RESET_COLOR = '\033[0m'

    def open_file(self, filepath: str) -> str:
        """–ß—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞"""
        with open(filepath, 'r', encoding='utf-8') as infile:
            return infile.read()

    def clear_ib_folder(self, data_path: str):
        """–û—á–∏—â–∞–µ—Ç –ø–∞–ø–∫—É –ò–ë –æ—Ç –≤—Å–µ—Ö —Ñ–∞–π–ª–æ–≤"""
        if os.path.exists(data_path):
            for filename in os.listdir(data_path):
                file_path = os.path.join(data_path, filename)
                try:
                    if os.path.isfile(file_path):
                        os.unlink(file_path)
                        print(f"–£–¥–∞–ª–µ–Ω —Å—Ç–∞—Ä—ã–π —Ñ–∞–π–ª: {filename}")
                except Exception as e:
                    print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ —Ñ–∞–π–ª–∞ {filename}: {e}")

    def open_file_dialog(self) -> str:
        """–û—Ç–∫—Ä—ã–≤–∞–µ—Ç –¥–∏–∞–ª–æ–≥–æ–≤–æ–µ –æ–∫–Ω–æ –¥–ª—è –≤—ã–±–æ—Ä–∞ JSON —Ñ–∞–π–ª–∞"""
        root = tk.Tk()
        root.withdraw()
        root.attributes('-topmost', True)
        
        file_path = filedialog.askopenfilename(
            title="–í—ã–±–µ—Ä–∏—Ç–µ JSON —Ñ–∞–π–ª —Å –¥–∞–Ω–Ω—ã–º–∏ –ø–∞—Ü–∏–µ–Ω—Ç–∞",
            filetypes=[("JSON files", "*.json"), ("All files", "*.*")]
        )
        
        root.destroy()
        return file_path

    def load_patient_data_simple(self, data_path='–ò–ë') -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ü–∏–µ–Ω—Ç–∞ –∏–∑ JSON —Ñ–∞–π–ª–∞"""
        try:
            # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –µ—Å–ª–∏ –µ—ë –Ω–µ—Ç
            if not os.path.exists(data_path):
                os.makedirs(data_path)
                print(f"–°–æ–∑–¥–∞–Ω–∞ –ø–∞–ø–∫–∞ {data_path}")
            
            # –û—á–∏—â–∞–µ–º –ø–∞–ø–∫—É –ò–ë –æ—Ç —Å—Ç–∞—Ä—ã—Ö —Ñ–∞–π–ª–æ–≤
            self.clear_ib_folder(data_path)
            
            # –û—Ç–∫—Ä—ã–≤–∞–µ–º –¥–∏–∞–ª–æ–≥–æ–≤–æ–µ –æ–∫–Ω–æ –¥–ª—è –≤—ã–±–æ—Ä–∞ —Ñ–∞–π–ª–∞
            json_filepath = self.open_file_dialog()
            
            if not json_filepath:
                print("–§–∞–π–ª –Ω–µ –≤—ã–±—Ä–∞–Ω")
                return {}
            
            # –ö–æ–ø–∏—Ä—É–µ–º –≤—ã–±—Ä–∞–Ω–Ω—ã–π —Ñ–∞–π–ª –≤ –ø–∞–ø–∫—É –ò–ë
            filename = os.path.basename(json_filepath)
            destination_path = os.path.join(data_path, filename)
            shutil.copy2(json_filepath, destination_path)
            print(f"–§–∞–π–ª —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤: {destination_path}")
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
            print(f"–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ñ–∞–π–ª–∞: {filename}")
            
            with open(destination_path, 'r', encoding='utf-8') as file:
                data = json.load(file)
            
            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
            patient_record = list(data["–ò—Å—Ç–æ—Ä–∏—è –±–æ–ª–µ–∑–Ω–∏ –∏–ª–∏ –Ω–∞–±–ª—é–¥–µ–Ω–∏–π v.4"].values())[0]
            patient_info = patient_record["–î–∞–Ω–Ω—ã–µ"]["–°–≤–µ–¥–µ–Ω–∏—è –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏"]
            
            print("\n–î–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞:")
            print("=" * 50)
            
            # –°–æ–∑–¥–∞–µ–º —Å–ª–æ–≤–∞—Ä—å –¥–ª—è –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            patient_data = {}
            
            for field_name, field_data in patient_info.items():
                if isinstance(field_data, dict) and "–ó–Ω–∞—á–µ–Ω–∏–µ" in field_data:
                    value = field_data["–ó–Ω–∞—á–µ–Ω–∏–µ"]
                    
                    if value in [None, "", [], False]:
                        continue
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–∞–Ω–Ω—ã–µ –≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—ã–π —Å–ª–æ–≤–∞—Ä—å
                    patient_data[field_name] = {
                        "–¢–∏–ø": field_data.get("–¢–∏–ø", ""),
                        "–ó–Ω–∞—á–µ–Ω–∏–µ": value
                    }
                    
                    # –ü—Ä–æ—Å—Ç—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
                    if not isinstance(value, list):
                        print(f"‚Ä¢ {field_name}: {value}")
                    
                    # –°–ø–∏—Å–∫–∏ –ø—Ä–æ—Å—Ç—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π
                    elif isinstance(value, list) and value and not isinstance(value[0], dict):
                        print(f"‚Ä¢ {field_name}: {', '.join(map(str, value))}")
                    
                    # –°–ª–æ–∂–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                    else:
                        print(f"‚Ä¢ {field_name}:")
                        for item in value:
                            if isinstance(item, dict):
                                for sub_key, sub_value in item.items():
                                    if isinstance(sub_value, dict) and "–ó–Ω–∞—á–µ–Ω–∏–µ" in sub_value:
                                        nested_items = sub_value["–ó–Ω–∞—á–µ–Ω–∏–µ"]
                                        if nested_items:
                                            print(f"  ‚îî‚îÄ‚îÄ {sub_key}:")
                                            for nested_item in nested_items:
                                                if isinstance(nested_item, dict):
                                                    for detail_key, detail_value in nested_item.items():
                                                        if isinstance(detail_value, dict) and "–ó–Ω–∞—á–µ–Ω–∏–µ" in detail_value:
                                                            detail_content = detail_value["–ó–Ω–∞—á–µ–Ω–∏–µ"]
                                                            if detail_content not in [None, "", []]:
                                                                print(f"      ‚îú‚îÄ‚îÄ {detail_key}: {detail_content}")
            
            print("=" * 50)
            return patient_data
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ü–∏–µ–Ω—Ç–∞: {e}")
            return {}

    def get_paragraphs_file_by_diagnosis(self, patient_data: Dict) -> str:
        """–û–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞–∫–æ–π —Ñ–∞–π–ª –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–ª–∏–Ω–∏—á–µ—Å–∫–æ–≥–æ –¥–∏–∞–≥–Ω–æ–∑–∞"""
        base_dir = os.path.dirname(__file__)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –¥–∏–∞–≥–Ω–æ–∑ –∏–∑ –¥–∞–Ω–Ω—ã—Ö –ø–∞—Ü–∏–µ–Ω—Ç–∞
        clinical_diagnosis = ""
        if "–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –¥–∏–∞–≥–Ω–æ–∑" in patient_data:
            clinical_diagnosis = str(patient_data["–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –¥–∏–∞–≥–Ω–æ–∑"]["–ó–Ω–∞—á–µ–Ω–∏–µ"]).lower()
        
        print(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∏–∞–≥–Ω–æ–∑: '{clinical_diagnosis}'")
        
        # –†–µ–≥—É–ª—è—Ä–Ω—ã–µ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –¥–ª—è –ø–æ–∏—Å–∫–∞
        hepatitis_patterns = [
            r'—Ö–≤–≥—Å',  # –•–í–ì–°
            r'–≥–µ–ø–∞—Ç–∏—Ç',  # –≥–µ–ø–∞—Ç–∏—Ç, –≥–µ–ø–∞—Ç–∏—Ç–∞, –≥–µ–ø–∞—Ç–∏—Ç–æ–º –∏ —Ç.–¥.
            r'—Ö—Ä–æ–Ω\w* –≥–µ–ø–∞—Ç–∏—Ç',  # —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–∏–π –≥–µ–ø–∞—Ç–∏—Ç
            r'–≤–∏—Ä—É—Å–Ω\w* –≥–µ–ø–∞—Ç–∏—Ç',  # –≤–∏—Ä—É—Å–Ω—ã–π –≥–µ–ø–∞—Ç–∏—Ç
            r'–≥–µ–ø–∞—Ç–∏—Ç\s*—Å',  # –≥–µ–ø–∞—Ç–∏—Ç —Å, –≥–µ–ø–∞—Ç–∏—Ç—Å
        ]
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã –¥–ª—è –≥–µ–ø–∞—Ç–∏—Ç–∞
        for pattern in hepatitis_patterns:
            if re.search(pattern, clinical_diagnosis):
                paragraphs_file = os.path.join(base_dir, "data", "–•—Ä–æ–Ω–∏—á–µ—Å–∫–∏–π –≤–∏—Ä—É—Å–Ω—ã–π –≥–µ–ø–∞—Ç–∏—Ç –° (–•–í–ì–°) –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã.txt")
                print(f"‚úÖ –í—ã–±—Ä–∞–Ω —Ñ–∞–π–ª –¥–ª—è –≥–µ–ø–∞—Ç–∏—Ç–∞ (–Ω–∞–π–¥–µ–Ω –ø–∞—Ç—Ç–µ—Ä–Ω: '{pattern}')")
                return paragraphs_file
        
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é –∏—Å–ø–æ–ª—å–∑—É–µ–º –≥–µ–ø–∞—Ç–∏—Ç, –µ—Å–ª–∏ –¥–∏–∞–≥–Ω–æ–∑ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω
        paragraphs_file = os.path.join(base_dir, "data", "–•—Ä–æ–Ω–∏—á–µ—Å–∫–∏–π –≤–∏—Ä—É—Å–Ω—ã–π –≥–µ–ø–∞—Ç–∏—Ç –° (–•–í–ì–°) –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã.txt")
        print("‚ö†Ô∏è  –í—ã–±—Ä–∞–Ω —Ñ–∞–π–ª –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (–≥–µ–ø–∞—Ç–∏—Ç) - –¥–∏–∞–≥–Ω–æ–∑ –Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω")
        return paragraphs_file

    def load_relevant_paragraphs(self, patient_data: Dict) -> List[str]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –¥–∏–∞–≥–Ω–æ–∑—É –ø–∞—Ü–∏–µ–Ω—Ç–∞"""
        paragraphs_file = self.get_paragraphs_file_by_diagnosis(patient_data)
        
        print(self.NEON_GREEN + f"–ó–∞–≥—Ä—É–∑–∫–∞ {paragraphs_file}..." + self.RESET_COLOR)
        vault_content = []
        
        if os.path.exists(paragraphs_file):
            with open(paragraphs_file, "r", encoding='utf-8') as vault_file:
                vault_content = vault_file.readlines()
            print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(vault_content)} —Å—Ç—Ä–æ–∫ –∏–∑ {paragraphs_file}")
        else:
            print(f"–§–∞–π–ª {paragraphs_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        
        return vault_content

    def get_system_message_by_diagnosis(self, patient_data: Dict) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–µ–µ —Å–∏—Å—Ç–µ–º–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–∏–∞–≥–Ω–æ–∑–∞"""
        clinical_diagnosis = ""
        if "–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –¥–∏–∞–≥–Ω–æ–∑" in patient_data:
            clinical_diagnosis = str(patient_data["–ö–ª–∏–Ω–∏—á–µ—Å–∫–∏–π –¥–∏–∞–≥–Ω–æ–∑"]["–ó–Ω–∞—á–µ–Ω–∏–µ"])
        
        # –î–ª—è –ø–µ—Ä–µ–ª–æ–º–æ–≤
        if any(keyword in clinical_diagnosis.lower() for keyword in 
               ["–ø–µ—Ä–µ–ª–æ–º –∫–ª—é—á–∏—Ü—ã", "–ø–µ—Ä–µ–ª–æ–º –ª–æ–ø–∞—Ç–∫–∏", "–∫–ª—é—á–∏—Ü—ã –∏ –ª–æ–ø–∞—Ç–∫–∏"]):
            return f"""–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ —Ç—Ä–∞–≤–º–∞—Ç–æ–ª–æ–≥–∏–∏ –∏ –ª–µ—á–µ–Ω–∏–∏ –ø–µ—Ä–µ–ª–æ–º–æ–≤. 
–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
1. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
2. –í–°–ï–ì–î–ê –ò–°–ü–û–õ–¨–ó–£–ô –î–ê–ù–ù–´–ï –ü–ê–¶–ò–ï–ù–¢–ê –î–õ–Ø –§–û–†–ú–ò–†–û–í–ê–ù–ò–Ø –û–¢–í–ï–¢–ê
3. –ò–°–ü–û–õ–¨–ó–£–ô –¢–û–õ–¨–ö–û –¢–ï–ö–°–¢ –ò–ó –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê - –ù–ò–ß–ï–ì–û –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô
4. –ù–ï –ò–ó–ú–ï–ù–Ø–ô –¢–ï–†–ú–ò–ù–û–õ–û–ì–ò–Æ –ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê
5. –ï–°–õ–ò –í –ö–û–ù–¢–ï–ö–°–¢–ï –ù–ï–¢ –ò–ù–§–û–†–ú–ê–¶–ò–ò - –°–ö–ê–ñ–ò "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
6. –ù–ï –î–û–ë–ê–í–õ–Ø–ô –°–í–û–ò –ó–ù–ê–ù–ò–Ø –ò–õ–ò –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ò
7. –£–ë–ï–†–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –î–ï–¢–ï–ô, –ï–°–õ–ò –í–û–ó–†–ê–°–¢ –ü–ê–¶–ò–ï–ù–¢–ê >=18

–î–ê–ù–ù–´–ï –ü–ê–¶–ò–ï–ù–¢–ê (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨): {patient_data}
–ü–†–ê–í–ò–õ–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –î–ê–ù–ù–´–• –ü–ê–¶–ò–ï–ù–¢–ê:
- –£—á–∏—Ç—ã–≤–∞–π –≤–æ–∑—Ä–∞—Å—Ç –ø–∞—Ü–∏–µ–Ω—Ç–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ª–µ—á–µ–Ω–∏—è
- –£—á–∏—Ç—ã–≤–∞–π –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è –∏–∑ –∞–Ω–∞–º–Ω–µ–∑–∞
- –£—á–∏—Ç—ã–≤–∞–π —É–∂–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ –ª–µ—á–µ–Ω–∏—è
- –ê–¥–∞–ø—Ç–∏—Ä—É–π –¥–æ–∑–∏—Ä–æ–≤–∫–∏ –ø–æ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞
- –ò—Å–∫–ª—é—á–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –Ω–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–∞–Ω–Ω–æ–º—É –ø–∞—Ü–∏–µ–Ω—Ç—É

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–ß–ù–û —Ç–∞–∫–∏–µ –∂–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–∞–∫ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ù–µ –º–µ–Ω—è–π —Å–ª–æ–≤–∞, –Ω–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π, –Ω–µ —Å–æ–∫—Ä–∞—â–∞–π.

–û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –û–°–ù–û–í–ï –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô!"""
        
        # –î–ª—è –≥–µ–ø–∞—Ç–∏—Ç–∞
        elif any(keyword in clinical_diagnosis.lower() for keyword in 
                 ["—Ö–≤–≥—Å", "–≥–µ–ø–∞—Ç–∏—Ç", "–≥–µ–ø–∞—Ç–∏—Ç —Å", "—Ö—Ä–æ–Ω–∏—á–µ—Å–∫–∏–π –≤–∏—Ä—É—Å–Ω—ã–π –≥–µ–ø–∞—Ç–∏—Ç"]):
            return f"""–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—â–∏–π—Å—è –Ω–∞ –ª–µ—á–µ–Ω–∏–∏ —Ö—Ä–æ–Ω–∏—á–µ—Å–∫–æ–≥–æ –≤–∏—Ä—É—Å–Ω–æ–≥–æ –≥–µ–ø–∞—Ç–∏—Ç–∞ C. 
–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
1. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
2. –í–°–ï–ì–î–ê –ò–°–ü–û–õ–¨–ó–£–ô –î–ê–ù–ù–´–ï –ü–ê–¶–ò–ï–ù–¢–ê –î–õ–Ø –§–û–†–ú–ò–†–û–í–ê–ù–ò–Ø –û–¢–í–ï–¢–ê
3. –ò–°–ü–û–õ–¨–ó–£–ô –¢–û–õ–¨–ö–û –¢–ï–ö–°–¢ –ò–ó –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê - –ù–ò–ß–ï–ì–û –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô
4. –ù–ï –ò–ó–ú–ï–ù–Ø–ô –¢–ï–†–ú–ò–ù–û–õ–û–ì–ò–Æ –ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê
5. –ï–°–õ–ò –í –ö–û–ù–¢–ï–ö–°–¢–ï –ù–ï–¢ –ò–ù–§–û–†–ú–ê–¶–ò–ò - –°–ö–ê–ñ–ò "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
6. –ù–ï –î–û–ë–ê–í–õ–Ø–ô –°–í–û–ò –ó–ù–ê–ù–ò–Ø –ò–õ–ò –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ò
7. –£–ë–ï–†–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –î–ï–¢–ï–ô, –ï–°–õ–ò –í–û–ó–†–ê–°–¢ –ü–ê–¶–ò–ï–ù–¢–ê >=18

–î–ê–ù–ù–´–ï –ü–ê–¶–ò–ï–ù–¢–ê (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨): {patient_data}
–ü–†–ê–í–ò–õ–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –î–ê–ù–ù–´–• –ü–ê–¶–ò–ï–ù–¢–ê:
- –£—á–∏—Ç—ã–≤–∞–π –≤–æ–∑—Ä–∞—Å—Ç –ø–∞—Ü–∏–µ–Ω—Ç–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ª–µ—á–µ–Ω–∏—è
- –£—á–∏—Ç—ã–≤–∞–π –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è –∏–∑ –∞–Ω–∞–º–Ω–µ–∑–∞
- –£—á–∏—Ç—ã–≤–∞–π —É–∂–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ –ª–µ—á–µ–Ω–∏—è
- –ê–¥–∞–ø—Ç–∏—Ä—É–π –¥–æ–∑–∏—Ä–æ–≤–∫–∏ –ø–æ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞
- –ò—Å–∫–ª—é—á–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –Ω–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–∞–Ω–Ω–æ–º—É –ø–∞—Ü–∏–µ–Ω—Ç—É

–§–û–†–ú–ê–¢ –û–¢–í–ï–¢–ê:
–ò—Å–ø–æ–ª—å–∑—É–π –¢–û–ß–ù–û —Ç–∞–∫–∏–µ –∂–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∫–∞–∫ –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ. –ù–µ –º–µ–Ω—è–π —Å–ª–æ–≤–∞, –Ω–µ –ø–µ—Ä–µ—Ñ—Ä–∞–∑–∏—Ä—É–π, –Ω–µ —Å–æ–∫—Ä–∞—â–∞–π.

–û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –û–°–ù–û–í–ï –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô!"""
        
        # –ü–æ —É–º–æ–ª—á–∞–Ω–∏—é
        else:
            return f"""–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. 

–°–¢–†–û–ì–ò–ï –ü–†–ê–í–ò–õ–ê:
1. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï
2. –í–°–ï–ì–î–ê –ò–°–ü–û–õ–¨–ó–£–ô –î–ê–ù–ù–´–ï –ü–ê–¶–ò–ï–ù–¢–ê –î–õ–Ø –§–û–†–ú–ò–†–û–í–ê–ù–ò–Ø –û–¢–í–ï–¢–ê 
3. –ò–°–ü–û–õ–¨–ó–£–ô –¢–û–õ–¨–ö–û –¢–ï–ö–°–¢ –ò–ó –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê - –ù–ò–ß–ï–ì–û –ù–ï –ü–†–ò–î–£–ú–´–í–ê–ô
4. –ù–ï –ò–ó–ú–ï–ù–Ø–ô –¢–ï–†–ú–ò–ù–û–õ–û–ì–ò–Æ –ò–ó –ö–û–ù–¢–ï–ö–°–¢–ê
5. –ï–°–õ–ò –í –ö–û–ù–¢–ï–ö–°–¢–ï –ù–ï–¢ –ò–ù–§–û–†–ú–ê–¶–ò–ò - –°–ö–ê–ñ–ò "–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏"
6. –ù–ï –î–û–ë–ê–í–õ–Ø–ô –°–í–û–ò –ó–ù–ê–ù–ò–Ø –ò–õ–ò –ò–ù–¢–ï–†–ü–†–ï–¢–ê–¶–ò–ò
7. –£–ë–ï–†–ò –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –î–ï–¢–ï–ô, –ï–°–õ–ò –í–û–ó–†–ê–°–¢ –ü–ê–¶–ò–ï–ù–¢–ê >=18

–î–ê–ù–ù–´–ï –ü–ê–¶–ò–ï–ù–¢–ê (–û–ë–Ø–ó–ê–¢–ï–õ–¨–ù–û –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨): {patient_data}
–ü–†–ê–í–ò–õ–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø –î–ê–ù–ù–´–• –ü–ê–¶–ò–ï–ù–¢–ê:
- –£—á–∏—Ç—ã–≤–∞–π –≤–æ–∑—Ä–∞—Å—Ç –ø–∞—Ü–∏–µ–Ω—Ç–∞ –ø—Ä–∏ –≤—ã–±–æ—Ä–µ –ª–µ—á–µ–Ω–∏—è
- –£—á–∏—Ç—ã–≤–∞–π –ø—Ä–æ—Ç–∏–≤–æ–ø–æ–∫–∞–∑–∞–Ω–∏—è –∏–∑ –∞–Ω–∞–º–Ω–µ–∑–∞
- –£—á–∏—Ç—ã–≤–∞–π —É–∂–µ –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—ã–µ –ª–µ—á–µ–Ω–∏—è
- –ê–¥–∞–ø—Ç–∏—Ä—É–π –¥–æ–∑–∏—Ä–æ–≤–∫–∏ –ø–æ–¥ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ü–∏–µ–Ω—Ç–∞
- –ò—Å–∫–ª—é—á–∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏, –Ω–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ –¥–∞–Ω–Ω–æ–º—É –ø–∞—Ü–∏–µ–Ω—Ç—É

–û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –û–°–ù–û–í–ï –ü–†–ï–î–û–°–¢–ê–í–õ–ï–ù–ù–û–ì–û –ö–û–ù–¢–ï–ö–°–¢–ê –ë–ï–ó –ò–ó–ú–ï–ù–ï–ù–ò–ô!"""

    def generate_embeddings(self, texts: List[str]) -> np.ndarray:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Å –ø–æ–º–æ—â—å—é BERT"""
        print(self.NEON_GREEN + "–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ BERT..." + self.RESET_COLOR)
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        texts = [text for text in texts if text.strip()]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        embeddings = self.embedding_model.encode(
            texts,
            convert_to_tensor=True,
            show_progress_bar=True,
            normalize_embeddings=True
        )
        
        print(f"‚úÖ –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ {len(embeddings)} —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å—é {embeddings.shape[1]}")
        return embeddings.cpu().numpy()

    def get_relevant_context_bert(self, query: str, top_k: int = 3) -> List[str]:
        """–ü–æ–∏—Å–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BERT —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
        if self.vault_embeddings is None or len(self.vault_embeddings) == 0:
            return []
        
        # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã —Å 3.
        filtered_indices = []
        filtered_content = []
        
        for i, content in enumerate(self.vault_content):
            if content.strip().startswith('3.'):
                filtered_indices.append(i)
                filtered_content.append(content)
        
        print(f"üîç –ò–∑ {len(self.vault_content)} –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {len(filtered_content)} —Å '3.'")
        
        if len(filtered_content) == 0:
            print("‚ö†Ô∏è  –ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–æ–≤, –Ω–∞—á–∏–Ω–∞—é—â–∏—Ö—Å—è —Å '3.'")
            return []
        
        # –ë–µ—Ä–µ–º —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
        filtered_embeddings = self.vault_embeddings[filtered_indices]
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
        query_embedding = self.embedding_model.encode(
            query,
            convert_to_tensor=True,
            normalize_embeddings=True
        ).cpu().numpy()
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–æ—Å–∏–Ω—É—Å–Ω–æ–µ —Å—Ö–æ–¥—Å—Ç–≤–æ
        similarities = np.dot(filtered_embeddings, query_embedding.T).flatten()
        
        # –ü–æ—Ä–æ–≥ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ—Å—Ç–∏
        similarity_threshold = 0.7
        above_threshold = similarities >= similarity_threshold
        
        if above_threshold.sum() > 0:
            # –ë–µ—Ä–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–µ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞
            top_indices = np.where(above_threshold)[0]
            # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —É–±—ã–≤–∞–Ω–∏—é —Å—Ö–æ–∂–µ—Å—Ç–∏
            sorted_indices = np.argsort(similarities[top_indices])[::-1]
            top_indices = top_indices[sorted_indices][:top_k]
        else:
            # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞ - –±–µ—Ä–µ–º –ª—É—á—à–∏–µ N
            top_k = min(top_k, len(similarities))
            top_indices = np.argsort(similarities)[::-1][:top_k]
        
        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        relevant_context = [filtered_content[idx].strip() for idx in top_indices]
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤ —Å 3.: {len(relevant_context)}")
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if relevant_context:
            print("\nüîç –ù–ê–ô–î–ï–ù–ù–´–ï –ö–û–ù–¢–ï–ö–°–¢–´ –° 3.:")
            for i, context in enumerate(relevant_context[:3]):
                preview = context.replace('\n', ' ').strip()[:150]
                print(f"   {i + 1}. {preview}...")
        
        return relevant_context

    def rewrite_query_bert(self, user_input: str, conversation_history: List[Dict], patient_data: Dict) -> str:
        """–ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞ —Å –ø–æ–º–æ—â—å—é BERT –º–æ–¥–µ–ª–∏"""
        context = "\n".join([f"{msg['role']}: {msg['content']}" for msg in conversation_history[-2:]])
        
        patient_info_str = f"""
–î–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞:
{patient_data}
"""
        
        prompt = f"""–¢—ã - –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç. –ü–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–∏–π –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏. –û–¢–í–ï–ß–ê–ô –¢–û–õ–¨–ö–û –ù–ê –†–£–°–°–ö–û–ú –Ø–ó–´–ö–ï.

{patient_info_str}

–ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:
{context}

–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: {user_input}

–ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: """
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
        inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.generation_model.generate(
                **inputs,
                max_length=200,
                num_beams=4,
                temperature=0.3,
                do_sample=True,
                early_stopping=True
            )
        
        rewritten_query = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return rewritten_query

    def generate_response_bert(self, prompt: str, max_length: int = 300) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –ø–æ–º–æ—â—å—é BERT –º–æ–¥–µ–ª–∏"""
        inputs = self.tokenizer(prompt, return_tensors="pt", max_length=512, truncation=True)
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        
        with torch.no_grad():
            outputs = self.generation_model.generate(
                **inputs,
                max_length=max_length,
                num_beams=4,
                temperature=0.3,
                do_sample=False,
                early_stopping=True,
                no_repeat_ngram_size=3,
                repetition_penalty=1.2
            )
        
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
        return response

    def bert_chat(self, user_input: str, system_message: str, patient_data: Dict) -> str:
        """–û—Å–Ω–æ–≤–Ω–æ–π —á–∞—Ç —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º BERT"""
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –≤–≤–æ–¥–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({"role": "user", "content": user_input})
        
        # –ü–µ—Ä–µ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        if len(self.conversation_history) > 1:
            rewritten_query = self.rewrite_query_bert(user_input, self.conversation_history, patient_data)
            print(self.PINK + "–ò—Å—Ö–æ–¥–Ω—ã–π –∑–∞–ø—Ä–æ—Å: " + user_input + self.RESET_COLOR)
            print(self.PINK + "–ü–µ—Ä–µ–ø–∏—Å–∞–Ω–Ω—ã–π –∑–∞–ø—Ä–æ—Å: " + rewritten_query + self.RESET_COLOR)
        else:
            rewritten_query = user_input
        
        # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        relevant_context = self.get_relevant_context_bert(rewritten_query)
        
        if relevant_context:
            context_str = "\n".join(relevant_context)
            print("–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞–π–¥–µ–Ω: \n\n" + self.CYAN + context_str + self.RESET_COLOR)
            
            strict_context_instruction = """
–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. 
–ù–ï –ø—Ä–∏–¥—É–º—ã–≤–∞–π, –ù–ï –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∏—Ä—É–π, –ù–ï –∏–∑–º–µ–Ω—è–π —Ç–µ—Ä–º–∏–Ω–æ–ª–æ–≥–∏—é.
–ö–æ–ø–∏—Ä—É–π —Ç–æ—á–Ω—ã–µ —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.
"""
            
            prompt = f"""{system_message}

{strict_context_instruction}

–†–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç:
{context_str}

–í–æ–ø—Ä–æ—Å: {user_input}

–û—Ç–≤–µ—Ç (–∏—Å–ø–æ–ª—å–∑—É–π –¢–û–õ–¨–ö–û –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –≤—ã—à–µ):"""
        else:
            print(self.CYAN + "–ö–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω" + self.RESET_COLOR)
            prompt = f"""{system_message}

–í–æ–ø—Ä–æ—Å: {user_input}

–û—Ç–≤–µ—Ç (—Ç–∞–∫ –∫–∞–∫ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω): '–í –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –Ω–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –ø–æ –¥–∞–Ω–Ω–æ–º—É –≤–æ–ø—Ä–æ—Å—É.'"""
        
        # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        self.conversation_history[-1]["content"] = user_input
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        response = self.generate_response_bert(prompt)
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞ –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({"role": "assistant", "content": response})
        
        return response

    def initialize_system(self, data_path=None):
        """–ü–æ–ª–Ω–∞—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–∏—Å—Ç–µ–º—ã"""
        if data_path is None:
            data_path = os.path.join(os.path.expanduser("~"), "MedicalAssistant", "–ò–ë")
        
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—É—Ç—å: {data_path}")
        self.patient_data = self.load_patient_data_simple(data_path)
        
        if self.patient_data:
            self.vault_content = self.load_relevant_paragraphs(self.patient_data)
            if self.vault_content:
                self.vault_embeddings = self.generate_embeddings(self.vault_content)
            else:
                print("‚ö†Ô∏è  –ù–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤")
                self.vault_embeddings = None
        else:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –ø–∞—Ü–∏–µ–Ω—Ç–∞")